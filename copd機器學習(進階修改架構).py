# -*- coding: utf-8 -*-
"""COPD機器學習(進階修改架構).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ud3tPtcaYAqKC3A-9C_yFSj9qKMfZ5X-
"""

!pip install fasttreeshap
!pip install shap

import os
import math
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from google.colab import files
import io
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score
import seaborn as sns
import time

# 檢查 CUDA
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"使用設備：{device}")

# 全局變量，用來保存每個檔案的特徵向量和標籤
file_level_data = []
file_level_labels = []

# 計算頭部與軀幹角度的函數
def calculate_head_torso_angle(x_head, y_head, x_shoulder, y_shoulder):
    delta_x = x_head - x_shoulder
    delta_y = y_head - y_shoulder
    angle_radians = math.atan2(delta_y, delta_x)
    return math.degrees(angle_radians)

# 計算相鄰時間點的角速度
def calculate_angular_velocity(angle_series, time_series):
    angular_velocity = np.diff(angle_series) / np.diff(time_series)
    angular_velocity = np.append(angular_velocity, 0)# 填充0，使其長度與原始數據一致
    return angular_velocity

# 計算相鄰角速度之間的變化率
def calculate_angular_velocity_change_rate(angular_velocity, time_series):
    angular_velocity_change_rate = np.diff(angular_velocity) / np.diff(time_series)
    angular_velocity_change_rate = np.append(angular_velocity_change_rate, 0)# 填充0，使其長度與原始數據一致
    return angular_velocity_change_rate

# 計算角度變化的函數
def calculate_angle_change(x1, y1, x2, y2, x3, y3):
    # 向量 a 從 (x1, y1) 指向 (x2, y2)
    a_x = x2 - x1
    a_y = y2 - y1
    # 向量 b 從 (x2, y2) 指向 (x3, y3)
    b_x = x3 - x2
    b_y = y3 - y2
    # 計算向量的點積
    dot_product = a_x * b_x + a_y * b_y
    # 計算向量的模
    magnitude_a = math.hypot(a_x, a_y)
    magnitude_b = math.hypot(b_x, b_y)
    if magnitude_a * magnitude_b == 0:
        return 0
    # 計算角度的餘弦值
    cos_angle = dot_product / (magnitude_a * magnitude_b)
    cos_angle = max(-1, min(1, cos_angle))  # 確保 cos_angle 在 [-1, 1] 之間
    # 返回角度變化（以度為單位）
    return math.acos(cos_angle) * (180 / math.pi)

# 計算擺臂角度的函數
def calculate_arm_angle(x_shoulder, y_shoulder, x_elbow, y_elbow, x_wrist, y_wrist):
    # 向量 a (肩到肘)
    a_x = x_elbow - x_shoulder
    a_y = y_elbow - y_shoulder
    # 向量 b (肘到手腕)
    b_x = x_wrist - x_elbow
    b_y = y_wrist - y_elbow
    # 向量的點積
    dot_product = a_x * b_x + a_y * b_y
    # 向量的模
    magnitude_a = math.hypot(a_x, a_y)
    magnitude_b = math.hypot(b_x, b_y)

    if magnitude_a == 0 or magnitude_b == 0:
        return 0

    # 計算角度的餘弦值
    cos_angle = dot_product / (magnitude_a * magnitude_b)
    cos_angle = max(-1, min(1, cos_angle))  # 確保值在 [-1, 1] 之間

    # 返回角度（弧度轉換為度數）
    return math.degrees(math.acos(cos_angle))

# #計算擺臂角度（根據行走方向選擇左臂或右臂）
def calculate_arm_angle_for_direction(df, directions):
    arm_angles = []
    for i in range(len(directions)):
        if directions[i] == "right":
            # 向右走：計算左臂
            x_shoulder = df['kpt_13'].astype(float)[i]  # 左肩
            y_shoulder = df['kpt_14'].astype(float)[i]
            x_elbow = df['kpt_15'].astype(float)[i]     # 左肘
            y_elbow = df['kpt_16'].astype(float)[i]
            x_wrist = df['kpt_17'].astype(float)[i]     # 左手腕
            y_wrist = df['kpt_18'].astype(float)[i]
        else:
            # 向左走：計算右臂
            x_shoulder = df['kpt_6'].astype(float)[i]   # 右肩
            y_shoulder = df['kpt_7'].astype(float)[i]
            x_elbow = df['kpt_8'].astype(float)[i]      # 右肘
            y_elbow = df['kpt_9'].astype(float)[i]
            x_wrist = df['kpt_10'].astype(float)[i]     # 右手腕
            y_wrist = df['kpt_11'].astype(float)[i]

        # 計算每一幀的角度
        angle = calculate_arm_angle(x_shoulder, y_shoulder, x_elbow, y_elbow, x_wrist, y_wrist)
        arm_angles.append(angle)

    return arm_angles

# 判斷病患的行走方向
def determine_walking_direction(df):
    """
    根據肩膀的水平位置判斷病患是向左走還是向右走，並返回行走方向。
    """
    x_shoulder_left = df['kpt_5'].astype(float)   # 左肩
    x_shoulder_right = df['kpt_6'].astype(float)  # 右肩
    directions = []
    for i in range(len(x_shoulder_left)):
        if x_shoulder_left[i] < x_shoulder_right[i]:
            directions.append("left")  # 病患向左走
        else:
            directions.append("right")  # 病患向右走
    return directions

#  重心移動距離
def calculate_center_of_mass_distance(x_shoulder, y_shoulder, x_hip_left, y_hip_left, x_hip_right, y_hip_right):
    # 計算左右髖部的中心點
    x_hip_center = (x_hip_left + x_hip_right) / 2
    y_hip_center = (y_hip_left + y_hip_right) / 2

    # 計算肩膀和髖部中心的中點，即重心
    center_x = (x_shoulder + x_hip_center) / 2
    center_y = (y_shoulder + y_hip_center) / 2

    return center_x, center_y

# 步距計算函數
def step_x(x1, x2):
    return abs(x2 - x1)

# 計算步幅標準差
def calculate_step_length_std(step_lengths):
    return np.std(step_lengths) if len(step_lengths) > 1 else 0

# 計算加速度
def calculate_acceleration(speeds, time_series):
    speeds = np.array(speeds)
    time_series = np.array(time_series)

    # 如果時間軸的差異為 0，則無法計算加速度，直接返回 0
    time_diff = np.diff(time_series)
    if np.any(time_diff == 0):
        return [0] * len(speeds)

    acceleration = np.diff(speeds) / time_diff
    acceleration = list(acceleration) + [0]  # 填充 0 使加速度與 speeds 長度一致
    return acceleration

# 定義計算髖關節角度的函數
def calculate_hip_angle(x_shoulder, y_shoulder, x_hip, y_hip, x_knee, y_knee):
    # 向量 a：髖部到肩部
    a_x = x_shoulder - x_hip
    a_y = y_shoulder - y_hip

    # 向量 b：髖部到膝蓋
    b_x = x_knee - x_hip
    b_y = y_knee - y_hip

    # 計算向量的點積
    dot_product = a_x * b_x + a_y * b_y

    # 計算向量的模
    magnitude_a = math.hypot(a_x, a_y)
    magnitude_b = math.hypot(b_x, b_y)

    # 如果模為0，返回0角度
    if magnitude_a == 0 or magnitude_b == 0:
        return 0

    # 計算角度的餘弦值
    cos_angle = dot_product / (magnitude_a * magnitude_b)
    cos_angle = max(-1, min(1, cos_angle))  # 確保 cos_angle 在 [-1, 1] 之間

    # 返回角度（弧度轉換為度數）
    return math.degrees(math.acos(cos_angle))

def extract_features_from_rows(df, file_name, label):
    """
    從文件中提取每行數據的特徵，並計算統計特徵。
    """
    # 初始化特徵列表
    head_torso_angles = []
    arm_angles = []
    step_lengths = []
    shoulder_hip_distances = []
    hip_angles = []
    angular_velocities = []  # 新增角速度
    center_of_mass_distances = []
    row_features = []
    previous_center_of_mass = None
    previous_angle = None
    time_series = df['time'].astype(float).values if 'time' in df.columns else np.arange(len(df))  # 確保時間序列存在

    # 確保所有需要的列為 pandas.Series 格式
    for col in [f'kpt_{i+1}' for i in range(20)]:
        if col in df.columns:
            df[col] = pd.Series(df[col])

    for i in range(len(df)):
        try:
            # 提取數據點
            x_head = (float(df['kpt_1'].iloc[i]) + float(df['kpt_2'].iloc[i])) / 2
            y_head = (float(df['kpt_3'].iloc[i]) + float(df['kpt_4'].iloc[i])) / 2
            x_shoulder = (float(df['kpt_5'].iloc[i]) + float(df['kpt_6'].iloc[i])) / 2
            y_shoulder = (float(df['kpt_7'].iloc[i]) + float(df['kpt_8'].iloc[i])) / 2
            x_elbow = float(df['kpt_9'].iloc[i])
            y_elbow = float(df['kpt_10'].iloc[i])
            x_wrist = float(df['kpt_11'].iloc[i])
            y_wrist = float(df['kpt_12'].iloc[i])
            x_hip_left = float(df['kpt_13'].iloc[i])
            y_hip_left = float(df['kpt_14'].iloc[i])
            x_hip_right = float(df['kpt_15'].iloc[i])
            y_hip_right = float(df['kpt_16'].iloc[i])
            x_knee = float(df['kpt_17'].iloc[i])
            y_knee = float(df['kpt_18'].iloc[i])

            # 計算基本特徵
            head_torso_angle = calculate_head_torso_angle(x_head, y_head, x_shoulder, y_shoulder)
            arm_angle = calculate_arm_angle(x_shoulder, y_shoulder, x_elbow, y_elbow, x_wrist, y_wrist)
            step_length = abs(x_hip_left - x_hip_right)
            hip_angle = calculate_hip_angle(
                x_shoulder, y_shoulder,
                (x_hip_left + x_hip_right) / 2, (y_hip_left + y_hip_right) / 2,
                x_knee, y_knee
            )
            shoulder_hip_distance = math.hypot(
                x_shoulder - (x_hip_left + x_hip_right) / 2,
                y_shoulder - (y_hip_left + y_hip_right) / 2
            )

            # 重心移動距離
            current_center_of_mass = calculate_center_of_mass_distance(
                x_shoulder, y_shoulder, x_hip_left, y_hip_left, x_hip_right, y_hip_right
            )
            if previous_center_of_mass is not None:
                com_distance = math.hypot(
                    current_center_of_mass[0] - previous_center_of_mass[0],
                    current_center_of_mass[1] - previous_center_of_mass[1]
                )
                center_of_mass_distances.append(com_distance)
            else:
                center_of_mass_distances.append(0)
            previous_center_of_mass = current_center_of_mass

            # 計算角速度
            if previous_angle is not None:
                angular_velocity = abs(head_torso_angle - previous_angle) / (time_series[i] - time_series[i-1])
                angular_velocities.append(angular_velocity)
            else:
                angular_velocities.append(0)
            previous_angle = head_torso_angle

            # 收集每行特徵
            row_features.append({
                'head_torso_angle': head_torso_angle,
                'arm_angle': arm_angle,
                'step_length': step_length,
                'hip_angle': hip_angle,
                'shoulder_hip_distance': shoulder_hip_distance,
                'angular_velocity': angular_velocities[-1],
                'center_of_mass_x': current_center_of_mass[0],
                'center_of_mass_y': current_center_of_mass[1],
                'center_of_mass_distance': center_of_mass_distances[-1],
                'head_to_hip_distance': math.hypot(x_head - (x_hip_left + x_hip_right) / 2, y_head - (y_hip_left + y_hip_right) / 2),
                'label': label,
                'file_name': file_name
            })

            # 收集文件級特徵
            head_torso_angles.append(head_torso_angle)
            arm_angles.append(arm_angle)
            step_lengths.append(step_length)
            shoulder_hip_distances.append(shoulder_hip_distance)
            hip_angles.append(hip_angle)

        except Exception as e:
            print(f"特徵提取錯誤於文件 {file_name}, 行 {i}: {e}")
            continue

    # 計算文件級統計特徵
    statistical_features = {
        'mean_head_torso_angle': np.mean(head_torso_angles) if head_torso_angles else 0,
        'std_head_torso_angle': np.std(head_torso_angles) if head_torso_angles else 0,
        'max_head_torso_angle': np.max(head_torso_angles) if head_torso_angles else 0,
        'min_head_torso_angle': np.min(head_torso_angles) if head_torso_angles else 0,
        'mean_arm_angle': np.mean(arm_angles) if arm_angles else 0,
        'std_arm_angle': np.std(arm_angles) if arm_angles else 0,
        'mean_step_length': np.mean(step_lengths) if step_lengths else 0,
        'std_step_length': np.std(step_lengths) if step_lengths else 0,
        'mean_shoulder_hip_distance': np.mean(shoulder_hip_distances) if shoulder_hip_distances else 0,
        'std_shoulder_hip_distance': np.std(shoulder_hip_distances) if shoulder_hip_distances else 0,
        'mean_hip_angle': np.mean(hip_angles) if hip_angles else 0,
        'std_hip_angle': np.std(hip_angles) if hip_angles else 0,
        'label': label,
        'file_name': file_name
    }

    return {
        'row_features': row_features,
        'statistical_features': statistical_features
    }

def upload_data_and_label_file_level(label):

    uploaded_files = files.upload()
    row_features = []  # 每行數據的特徵
    statistical_features = []  # 文件級別統計特徵

    for file_name in uploaded_files.keys():
        # 讀取文件
        df = pd.read_csv(io.BytesIO(uploaded_files[file_name]))

        # 處理 'kpts' 列
        if 'kpts' in df.columns:
            df['kpts'] = df['kpts'].str.replace('"', '').str.strip()
            kpts_columns = df['kpts'].str.split(',', expand=True)
            kpts_columns.rename(columns=lambda x: f'kpt_{x+1}', inplace=True)
            df = pd.concat([df, kpts_columns], axis=1)
            for col in kpts_columns.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

        # 提取特徵
        features = extract_features_from_rows(df, file_name, label)

        # `extract_features_from_rows` 返回的應是一個包含兩個部分的字典
        row_features.extend(features["row_features"])  # 行級特徵
        statistical_features.append(features["statistical_features"])  # 文件級統計特徵

    return row_features, statistical_features

all_features = []
all_statistical_features = []
all_row_features = []  # 行級別特徵
all_statistical_features = []  # 文件級別統計特徵

# 上傳第一批數據並指定標籤為1
print("上傳第一批數據並指定標籤為1")
row_features, statistical_features = upload_data_and_label_file_level(1)
all_row_features.extend(row_features)
all_statistical_features.extend(statistical_features)

# 上傳第二批數據並指定標籤為2
print("上傳第二批數據並指定標籤為2")
row_features, statistical_features = upload_data_and_label_file_level(2)
all_row_features.extend(row_features)
all_statistical_features.extend(statistical_features)

# 上傳第三批數據並指定標籤為3
print("上傳第三批數據並指定標籤為3")
row_features, statistical_features = upload_data_and_label_file_level(3)
all_row_features.extend(row_features)
all_statistical_features.extend(statistical_features)

# 上傳第四批數據並指定標籤為4
print("上傳第四批數據並指定標籤為4")
row_features, statistical_features = upload_data_and_label_file_level(4)
all_row_features.extend(row_features)
all_statistical_features.extend(statistical_features)

print(f"新增特徵數量: {len(row_features[0].keys())}")
print(f"特徵名稱: {list(row_features[0].keys())}")

# 確認提取的特徵數量
print(f"提取的行級特徵數量: {len(all_row_features)}")
print(f"提取的文件級別統計特徵數量: {len(all_statistical_features)}")

# 將行級特徵轉換為 DataFrame
df_row_features = pd.DataFrame(all_row_features)

# 將統計特徵轉換為 DataFrame
df_statistical_features = pd.DataFrame(all_statistical_features)

if 'label' not in df_row_features.columns:
    raise ValueError("行級特徵缺少 label 欄位")

if 'label' not in df_statistical_features.columns:
    raise ValueError("統計特徵缺少 label 欄位")

# 將標籤從 1, 2, 3, 4 轉換到 0, 1, 2, 3
df_statistical_features['label'] = df_statistical_features['label'] - 1

# 保存為 CSV
df_row_features.to_csv('row_features_with_labels.csv', index=False)
df_statistical_features.to_csv('statistical_features_with_labels.csv', index=False)

# 下載文件
files.download('row_features_with_labels.csv')
files.download('statistical_features_with_labels.csv')

"""監督式學習"""

def split_data_by_label(df, test_size=0.3, random_state=42):

    train_dfs = []
    test_dfs = []

    # 按標籤分組處理
    for label, group in df.groupby('label'):
        # 提取唯一文件名
        unique_files = group[['file_name', 'label']].drop_duplicates()

        # 使用文件名進行分層抽樣
        train_files, test_files = train_test_split(
            unique_files,
            test_size=test_size,
            random_state=random_state,
            stratify=unique_files['label']
        )

        # 按文件名分配數據
        train_df = group[group['file_name'].isin(train_files['file_name'])]
        test_df = group[group['file_name'].isin(test_files['file_name'])]

        train_dfs.append(train_df)
        test_dfs.append(test_df)

    # 合併所有標籤的訓練集和測試集
    train_df = pd.concat(train_dfs, ignore_index=True)
    test_df = pd.concat(test_dfs, ignore_index=True)

    return train_df, test_df

print(df_row_features['label'].isnull().sum())  # 確認標籤是否有缺失
print(df_row_features['label'].unique())

def split_data_by_file(features_list, test_size=0.3, random_state=42):

    # 轉換為 DataFrame，方便操作
    df = pd.DataFrame(features_list)

    # 獲取每個文件的唯一標籤
    unique_files = df[['file_name', 'label']].drop_duplicates()

    # 檢查每個類別的文件數量
    file_counts = unique_files['label'].value_counts()
    insufficient_classes = file_counts[file_counts < 2].index.tolist()

    if insufficient_classes:
        print(f"以下類別的文件數不足，將分配全部數據到訓練集: {insufficient_classes}")
        sufficient_files = unique_files[~unique_files['label'].isin(insufficient_classes)]
        insufficient_files = unique_files[unique_files['label'].isin(insufficient_classes)]
    else:
        sufficient_files = unique_files
        insufficient_files = pd.DataFrame(columns=unique_files.columns)

    # 分層切分文件名
    train_files, test_files = train_test_split(
        sufficient_files, test_size=test_size, random_state=random_state, stratify=sufficient_files['label']
    )

    # 將單文件類別全部分配到訓練集中
    train_files = pd.concat([train_files, insufficient_files], ignore_index=True)

    # 按文件名分配行數據
    train_features = df[df['file_name'].isin(train_files['file_name'])]
    test_features = df[df['file_name'].isin(test_files['file_name'])]

    return train_features, test_features

# 建立神經網絡
class SeverityClassifier(nn.Module):
    def __init__(self, input_size, num_classes=4):
        super(SeverityClassifier, self).__init__()
        self.fc1 = nn.Linear(input_size, 32)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Linear(32, 64)
        self.relu2 = nn.ReLU()
        self.fc3 = nn.Linear(64, 16)
        self.relu3 = nn.ReLU()
        self.fc4 = nn.Linear(16, num_classes)

    def forward(self, x):
        x = self.relu1(self.fc1(x))
        x = self.relu2(self.fc2(x))
        x = self.relu3(self.fc3(x))
        x = self.fc4(x)
        return x

# 分割數據
train_df, test_df = split_data_by_file(all_statistical_features)

# 準備數據
X_train = train_df.drop(columns=['label', 'file_name']).to_numpy()
y_train = train_df['label'].to_numpy()
X_test = test_df.drop(columns=['label', 'file_name']).to_numpy()
y_test = test_df['label'].to_numpy()

# 調整範圍從 [1, 4] 到 [0, 3]
y_train = y_train - 1
y_test = y_test - 1

# 檢查數據是否包含缺失值
if np.isnan(X_train).any() or np.isnan(X_test).any():
    print("警告：數據集中包含缺失值！請檢查數據處理流程。")

X_train = np.nan_to_num(X_train)
X_test = np.nan_to_num(X_test)

# 確保數據類型為 float32
X_train = X_train.astype(np.float32)
X_test = X_test.astype(np.float32)



# 初始化模型
input_size = X_train.shape[1]
model = SeverityClassifier(input_size=input_size, num_classes=4).to(device)

# 定義損失函數和優化器
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)

# 訓練模型
def train_model(num_epochs=100):
    model.train()
    for epoch in range(num_epochs):
        for features, labels in train_loader:
            features, labels = features.to(device), labels.to(device)
            outputs = model(features)
            loss = criterion(outputs, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# 測試模型
def test_model():
    model.eval()
    true_labels, predicted_labels = [], []
    with torch.no_grad():
        for features, labels in test_loader:
            features, labels = features.to(device), labels.to(device)
            outputs = model(features)
            _, predicted = torch.max(outputs.data, 1)
            true_labels.extend(labels.cpu().numpy())
            predicted_labels.extend(predicted.cpu().numpy())
    return true_labels, predicted_labels

# 建立數據集類別
class PatientDataset(Dataset):
    def __init__(self, data, labels):
        self.features = data
        self.labels = labels

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        x = self.features[idx]
        y = self.labels[idx]
        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.long)

# 進行訓練和測試
train_dataset = PatientDataset(X_train, y_train)
test_dataset = PatientDataset(X_test, y_test)
train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)

print(f"X_train 形狀: {X_train.shape}, 特徵數量: {X_train.shape[1]}")
print(f"X_test 形狀: {X_test.shape}, 特徵數量: {X_test.shape[1]}")

print(f"模型輸入大小: {input_size}, 特徵數量: {X_train.shape[1]}")

# 訓練模型
train_model(num_epochs=200)

# 測試模型
true_labels, predicted_labels = test_model()

def test_model_with_thresholds():
    model.eval()
    start_time = time.time()
    true_labels = []
    predicted_labels = []
    with torch.no_grad():
        for features, labels in test_loader:
            features = features.to(device)
            labels = labels.to(device)

            # 模型輸出是每個類別的分數，使用 softmax 轉換為機率
            outputs = model(features)
            probabilities = torch.softmax(outputs, dim=1)

            # 取得每個樣本的最大機率及其對應的類別
            max_probs, predicted = torch.max(probabilities, 1)

            true_labels.extend(labels.cpu().numpy())
            predicted_labels.append((max_probs.cpu().numpy(), predicted.cpu().numpy()))

    end_time = time.time()
    total_time = end_time - start_time

    print(f'模型測試總共花費的時間: {total_time:.2f} 秒')

    # 計算不同閾值的準確率
    thresholds = np.arange(0.01, 1.01, 0.01)
    for threshold in thresholds:
        # 根據不同的閾值計算預測標籤
        adjusted_predictions = [
            pred if prob >= threshold else -1  # 將低於閾值的樣本標記為 -1
            for (probs, preds) in predicted_labels
            for prob, pred in zip(probs, preds)
        ]

        # 過濾掉標記為 -1 的樣本
        valid_indices = [i for i, pred in enumerate(adjusted_predictions) if pred != -1]
        filtered_true_labels = [true_labels[i] for i in valid_indices]
        filtered_predictions = [adjusted_predictions[i] for i in valid_indices]

        # 如果沒有有效的樣本，跳過這個閾值
        if len(filtered_true_labels) == 0:
            print(f'閾值 {threshold:.2f} 下沒有有效的樣本。')
            continue

        # 計算準確率
        accuracy = 100 * np.sum(np.array(filtered_true_labels) == np.array(filtered_predictions)) / len(filtered_true_labels)
        print(f'閾值 {threshold:.2f} 下的準確率: {accuracy:.2f}%')

test_model_with_thresholds()

# 評估模型的精確率、召回率和 F1 分數
def evaluate_model(true_labels, predicted_labels):
    # 計算精確率、召回率和 F1 分數
    precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=0)
    recall = recall_score(true_labels, predicted_labels, average='weighted')
    f1 = f1_score(true_labels, predicted_labels, average='weighted')

    # 顯示結果
    print(f'精確率（Precision）: {precision:.2f}')
    print(f'召回率（Recall）: {recall:.2f}')
    print(f'F1 分數: {f1:.2f}')

# 測試模型並獲取真實標籤和預測標籤
true_labels, predicted_labels = test_model()

# 評估模型的精確率、召回率和F1分數
evaluate_model(true_labels, predicted_labels)

"""閾值0.95"""

# 測試模型並應用閥值
def test_model_with_threshold(threshold=0.81):
    model.eval()
    true_labels = []
    predicted_labels = []
    with torch.no_grad():
        for features, labels in test_loader:
            features = features.to(device)
            labels = labels.to(device)

            # 模型輸出轉為機率
            outputs = model(features)
            probabilities = torch.softmax(outputs, dim=1)

            # 取得每個樣本的最大機率及其對應類別
            max_probs, predicted = torch.max(probabilities, dim=1)

            # 應用閥值篩選
            adjusted_predictions = [
                pred.item() if prob.item() >= threshold else -1  # 未達到閥值則標記為 -1
                for prob, pred in zip(max_probs, predicted)
            ]

            true_labels.extend(labels.cpu().numpy())
            predicted_labels.extend(adjusted_predictions)

    return true_labels, predicted_labels

# 評估模型在特定閥值下的性能
def evaluate_model_with_threshold(true_labels, predicted_labels, threshold=0.95):
    # 過濾出未被閾值排除的樣本
    valid_indices = [i for i, pred in enumerate(predicted_labels) if pred != -1]
    filtered_true_labels = [true_labels[i] for i in valid_indices]
    filtered_predictions = [predicted_labels[i] for i in valid_indices]

    if len(filtered_true_labels) == 0:
        print(f"閥值 {threshold:.2f} 下無有效樣本。")
        return

    # 計算精確率、召回率和 F1 分數
    precision = precision_score(filtered_true_labels, filtered_predictions, average='weighted', zero_division=0)
    recall = recall_score(filtered_true_labels, filtered_predictions, average='weighted')
    f1 = f1_score(filtered_true_labels, filtered_predictions, average='weighted')

    # 顯示結果
    print(f"在閥值 {threshold:.2f} 下的性能表現：")
    print(f"精確率（Precision）: {precision:.2f}")
    print(f"召回率（Recall）: {recall:.2f}")
    print(f"F1 分數: {f1:.2f}")

    # 返回評估指標
    return precision, recall, f1

threshold = 0.81
true_labels, predicted_labels = test_model_with_threshold(threshold=threshold)

evaluate_model_with_threshold(true_labels, predicted_labels, threshold=threshold)

def test_model_with_thresholds(threshold=0.81):
    model.eval()
    true_labels = []
    predicted_labels = []

    with torch.no_grad():
        for features, labels in test_loader:
            features, labels = features.to(device), labels.to(device)

            # 使用模型進行預測
            outputs = model(features)
            probabilities = torch.softmax(outputs, dim=1)

            # 獲取最大機率和預測標籤
            max_probs, preds = torch.max(probabilities, 1)
            true_labels.extend(labels.cpu().numpy())

            # 根據閾值調整預測
            adjusted_preds = [pred if prob >= threshold else -1 for prob, pred in zip(max_probs.cpu().numpy(), preds.cpu().numpy())]
            predicted_labels.extend(adjusted_preds)

    return true_labels, predicted_labels

# 使用 0.95 閾值進行測試
threshold = 0.81
true_labels, predicted_labels = test_model_with_thresholds(threshold)

def evaluate_and_plot_confusion_matrix(true_labels, predicted_labels, threshold):
    # 過濾掉閾值以下的未分類樣本 (-1)
    valid_indices = [i for i, pred in enumerate(predicted_labels) if pred != -1]
    filtered_true_labels = [true_labels[i] for i in valid_indices]
    filtered_predicted_labels = [predicted_labels[i] for i in valid_indices]

    # 檢查過濾後的標籤唯一值
    print("Filtered unique true labels:", np.unique(filtered_true_labels))
    print("Filtered unique predicted labels:", np.unique(filtered_predicted_labels))

    # 計算混淆矩陣
    cm = confusion_matrix(filtered_true_labels, filtered_predicted_labels, labels=[0, 1, 2, 3])

    # 繪製混淆矩陣
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1, 2, 3], yticklabels=[0, 1, 2, 3])
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title(f'Confusion Matrix (Threshold: {threshold:.2f})')
    plt.show()

    # 返回混淆矩陣供檢查
    return cm

cm = evaluate_and_plot_confusion_matrix(true_labels, predicted_labels, threshold=0.81)

"""50%混淆矩陣"""

# 繪製混淆矩陣
def plot_confusion_matrix(true_labels, predicted_labels):
    # 生成混淆矩陣
    cm = confusion_matrix(true_labels, predicted_labels)

    # 可視化混淆矩陣
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1, 2, 3], yticklabels=[0, 1, 2, 3])
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()

# 繪製混淆矩陣
plot_confusion_matrix(true_labels, predicted_labels)

import shap
import fasttreeshap
# 訓練 RandomForestClassifier 模型
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=200, max_depth=8, random_state=42)
rf.fit(X_train, y_train)

# 使用 FastTreeSHAP 計算 SHAP 值
explainer = fasttreeshap.TreeExplainer(rf)
shap_values = explainer.shap_values(X_test)

feature_names = [
    'mean_head_torso_angle', 'std_head_torso_angle', 'max_head_torso_angle', 'min_head_torso_angle',
    'mean_arm_angle', 'std_arm_angle', 'max_arm_angle', 'min_arm_angle',
    'mean_step_length', 'std_step_length', 'max_step_length', 'min_step_length',
    'mean_acceleration', 'std_acceleration', 'acceleration_rms',
    'step_frequency',
    'step_length_variation_coefficient',
    'mean_shoulder_hip_distance', 'std_shoulder_hip_distance', 'max_shoulder_hip_distance', 'min_shoulder_hip_distance',
    'mean_hip_angle', 'std_hip_angle', 'max_hip_angle', 'min_hip_angle',
    'mean_hip_angular_velocity', 'std_hip_angular_velocity', 'max_hip_angular_velocity', 'min_hip_angular_velocity',
    'mean_hip_angular_velocity_change_rate', 'std_hip_angular_velocity_change_rate', 'max_hip_angular_velocity_change_rate', 'min_hip_angular_velocity_change_rate',
    'mean_center_of_mass_distance', 'std_center_of_mass_distance', 'max_center_of_mass_distance', 'min_center_of_mass_distance'
]

# 可視化 SHAP 值
shap.summary_plot(shap_values, X_test, feature_names=feature_names)